{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasisvaran/Deep-Learning-Papers-Reading-Roadmap/blob/master/week7/assignment/Assignment_03_Ex_A_Vocab_Size_2000_Seq_Len_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg8FIQdIARYk"
      },
      "source": [
        "## MSDS458 Research Assignment 3:\n",
        "### Submitted by: Vasu (Vasisvaran) Gopal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydgzc1l15ukl"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9d9VZa_T5ukm"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as k\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNt8VbLK5ukp"
      },
      "source": [
        "## Verify TensorFlow version and Keras version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kujC9adr5ukq",
        "outputId": "d7dd60df-2be5-4da7-9163-a013fc207d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This notebook requires TensorFlow 2.0 or above\n",
            "TensorFlow version:  2.11.0\n"
          ]
        }
      ],
      "source": [
        "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHdPTZRp5ukr",
        "outputId": "34d2c212-cd97-4657-fc69-34d90640aec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version:  2.11.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Keras version: \", keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0LzteD3lBm"
      },
      "source": [
        "## Stopword Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a-kg8gCi3lBn"
      },
      "outputs": [],
      "source": [
        "def custom_stopwords(input_text):\n",
        "    lowercase = tf.strings.lower(input_text)\n",
        "    stripped_punct = tf.strings.regex_replace(lowercase\n",
        "                                  ,'[%s]' % re.escape(string.punctuation)\n",
        "                                  ,'')\n",
        "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvqFLia1Rro9"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsypDzDARro-",
        "outputId": "fed6d47d-fd38-45ca-d149-2b9aff717202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W0221 02:49:33.119301 140350607775552 download_and_prepare.py:46] ***`tfds build` should be used instead of `download_and_prepare`.***\n",
            "INFO[build.py]: Loading dataset ag_news_subset from imports: tensorflow_datasets.datasets.ag_news_subset.ag_news_subset_dataset_builder\n",
            "2023-02-21 02:49:34.032427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-21 02:49:34.032524: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-21 02:49:34.032543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-21 02:49:35.194725: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[dataset_info.py]: Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ag_news_subset/1.0.0\n",
            "INFO[dataset_info.py]: Load dataset info from /tmp/tmpqh6txl7dtfds\n",
            "INFO[dataset_info.py]: Field info.description from disk and from code do not match. Keeping the one from code.\n",
            "INFO[dataset_info.py]: Field info.splits from disk and from code do not match. Keeping the one from code.\n",
            "INFO[dataset_info.py]: Field info.supervised_keys from disk and from code do not match. Keeping the one from code.\n",
            "INFO[dataset_info.py]: Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
            "INFO[build.py]: download_and_prepare for dataset ag_news_subset/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset ag_news_subset (/root/tensorflow_datasets/ag_news_subset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset 11.24 MiB (download: 11.24 MiB, generated: 35.79 MiB, total: 47.03 MiB) to /root/tensorflow_datasets/ag_news_subset/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "                                       \n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[AINFO[download_manager.py]: Downloading https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms into /root/tensorflow_datasets/downloads/ucexport_download_id_0Bz8a_Dbh9QhbUDNpeUdjb0wxj4g1umFAV8OV-uDwxSJR0LdxO_k1jxMuFWwAfNX9jos.tmp.379f0c6694ae42799e86dd568e318b2c...\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:   0% 0/11 [00:16<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:16, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:   9% 1/11 [00:16<02:46, 16.60s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  18% 2/11 [00:16<02:29, 16.60s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  27% 3/11 [00:16<02:12, 16.60s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  36% 4/11 [00:16<01:56, 16.60s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:16, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  45% 5/11 [00:16<00:14,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  55% 6/11 [00:16<00:12,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  64% 7/11 [00:16<00:09,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  73% 8/11 [00:16<00:07,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  82% 9/11 [00:16<00:04,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...:  91% 10/11 [00:16<00:02,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:16<?, ? url/s]\n",
            "Dl Size...: 100% 11/11 [00:16<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:16<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:16<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:16<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:16<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/4 [00:17<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 16.76s/ url]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  2.49s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 4/4 [00:17<00:00,  4.25s/ file]\n",
            "Dl Size...: 100% 11/11 [00:17<00:00,  1.55s/ MiB]\n",
            "Dl Completed...: 100% 1/1 [00:17<00:00, 17.02s/ url]\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating train examples...:   0% 0/120000 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating train examples...:   1% 979/120000 [00:00<00:12, 9781.89 examples/s]\u001b[A\n",
            "Generating train examples...:   2% 1968/120000 [00:00<00:11, 9840.32 examples/s]\u001b[A\n",
            "Generating train examples...:   2% 2980/120000 [00:00<00:11, 9963.76 examples/s]\u001b[A\n",
            "Generating train examples...:   3% 3977/120000 [00:00<00:11, 9885.24 examples/s]\u001b[A\n",
            "Generating train examples...:   4% 4992/120000 [00:00<00:11, 9978.53 examples/s]\u001b[A\n",
            "Generating train examples...:   5% 6008/120000 [00:00<00:11, 10039.52 examples/s]\u001b[A\n",
            "Generating train examples...:   6% 7017/120000 [00:00<00:11, 10052.80 examples/s]\u001b[A\n",
            "Generating train examples...:   7% 8034/120000 [00:00<00:11, 10087.54 examples/s]\u001b[A\n",
            "Generating train examples...:   8% 9043/120000 [00:00<00:11, 10050.50 examples/s]\u001b[A\n",
            "Generating train examples...:   8% 10049/120000 [00:01<00:11, 9992.47 examples/s]\u001b[A\n",
            "Generating train examples...:   9% 11060/120000 [00:01<00:10, 10027.07 examples/s]\u001b[A\n",
            "Generating train examples...:  10% 12075/120000 [00:01<00:10, 10062.46 examples/s]\u001b[A\n",
            "Generating train examples...:  11% 13082/120000 [00:01<00:10, 9783.58 examples/s] \u001b[A\n",
            "Generating train examples...:  12% 14063/120000 [00:01<00:10, 9636.45 examples/s]\u001b[A\n",
            "Generating train examples...:  13% 15071/120000 [00:01<00:10, 9765.05 examples/s]\u001b[A\n",
            "Generating train examples...:  13% 16076/120000 [00:01<00:10, 9847.59 examples/s]\u001b[A\n",
            "Generating train examples...:  14% 17084/120000 [00:01<00:10, 9914.79 examples/s]\u001b[A\n",
            "Generating train examples...:  15% 18077/120000 [00:01<00:10, 9872.64 examples/s]\u001b[A\n",
            "Generating train examples...:  16% 19091/120000 [00:01<00:10, 9949.18 examples/s]\u001b[A\n",
            "Generating train examples...:  17% 20090/120000 [00:02<00:10, 9958.56 examples/s]\u001b[A\n",
            "Generating train examples...:  18% 21092/120000 [00:02<00:09, 9976.09 examples/s]\u001b[A\n",
            "Generating train examples...:  18% 22099/120000 [00:02<00:09, 10001.48 examples/s]\u001b[A\n",
            "Generating train examples...:  19% 23106/120000 [00:02<00:09, 10019.95 examples/s]\u001b[A\n",
            "Generating train examples...:  20% 24109/120000 [00:02<00:09, 9998.71 examples/s] \u001b[A\n",
            "Generating train examples...:  21% 25109/120000 [00:02<00:09, 9959.97 examples/s]\u001b[A\n",
            "Generating train examples...:  22% 26107/120000 [00:02<00:09, 9963.35 examples/s]\u001b[A\n",
            "Generating train examples...:  23% 27110/120000 [00:02<00:09, 9982.45 examples/s]\u001b[A\n",
            "Generating train examples...:  23% 28123/120000 [00:02<00:09, 10024.40 examples/s]\u001b[A\n",
            "Generating train examples...:  24% 29141/120000 [00:02<00:09, 10068.77 examples/s]\u001b[A\n",
            "Generating train examples...:  25% 30148/120000 [00:03<00:08, 10010.59 examples/s]\u001b[A\n",
            "Generating train examples...:  26% 31150/120000 [00:03<00:08, 10011.77 examples/s]\u001b[A\n",
            "Generating train examples...:  27% 32158/120000 [00:03<00:08, 10030.90 examples/s]\u001b[A\n",
            "Generating train examples...:  28% 33162/120000 [00:03<00:08, 10013.44 examples/s]\u001b[A\n",
            "Generating train examples...:  28% 34175/120000 [00:03<00:08, 10045.40 examples/s]\u001b[A\n",
            "Generating train examples...:  29% 35180/120000 [00:03<00:08, 9979.10 examples/s] \u001b[A\n",
            "Generating train examples...:  30% 36182/120000 [00:03<00:08, 9988.78 examples/s]\u001b[A\n",
            "Generating train examples...:  31% 37191/120000 [00:03<00:08, 10015.48 examples/s]\u001b[A\n",
            "Generating train examples...:  32% 38203/120000 [00:03<00:08, 10044.26 examples/s]\u001b[A\n",
            "Generating train examples...:  33% 39208/120000 [00:03<00:08, 10015.55 examples/s]\u001b[A\n",
            "Generating train examples...:  34% 40210/120000 [00:04<00:07, 9996.65 examples/s] \u001b[A\n",
            "Generating train examples...:  34% 41210/120000 [00:04<00:07, 9989.10 examples/s]\u001b[A\n",
            "Generating train examples...:  35% 42215/120000 [00:04<00:07, 10006.22 examples/s]\u001b[A\n",
            "Generating train examples...:  36% 43216/120000 [00:04<00:07, 10001.22 examples/s]\u001b[A\n",
            "Generating train examples...:  37% 44217/120000 [00:04<00:07, 9992.27 examples/s] \u001b[A\n",
            "Generating train examples...:  38% 45226/120000 [00:04<00:07, 10019.27 examples/s]\u001b[A\n",
            "Generating train examples...:  39% 46234/120000 [00:04<00:07, 10035.69 examples/s]\u001b[A\n",
            "Generating train examples...:  39% 47238/120000 [00:04<00:07, 9978.00 examples/s] \u001b[A\n",
            "Generating train examples...:  40% 48236/120000 [00:04<00:07, 9974.83 examples/s]\u001b[A\n",
            "Generating train examples...:  41% 49243/120000 [00:04<00:07, 10002.70 examples/s]\u001b[A\n",
            "Generating train examples...:  42% 50244/120000 [00:05<00:06, 9967.00 examples/s] \u001b[A\n",
            "Generating train examples...:  43% 51249/120000 [00:05<00:06, 9989.32 examples/s]\u001b[A\n",
            "Generating train examples...:  44% 52248/120000 [00:05<00:06, 9968.66 examples/s]\u001b[A\n",
            "Generating train examples...:  44% 53245/120000 [00:05<00:06, 9729.67 examples/s]\u001b[A\n",
            "Generating train examples...:  45% 54247/120000 [00:05<00:06, 9812.36 examples/s]\u001b[A\n",
            "Generating train examples...:  46% 55260/120000 [00:05<00:06, 9903.30 examples/s]\u001b[A\n",
            "Generating train examples...:  47% 56270/120000 [00:05<00:06, 9960.13 examples/s]\u001b[A\n",
            "Generating train examples...:  48% 57279/120000 [00:05<00:06, 9997.12 examples/s]\u001b[A\n",
            "Generating train examples...:  49% 58289/120000 [00:05<00:06, 10026.41 examples/s]\u001b[A\n",
            "Generating train examples...:  49% 59300/120000 [00:05<00:06, 10050.31 examples/s]\u001b[A\n",
            "Generating train examples...:  50% 60306/120000 [00:06<00:05, 9994.54 examples/s] \u001b[A\n",
            "Generating train examples...:  51% 61318/120000 [00:06<00:05, 10029.66 examples/s]\u001b[A\n",
            "Generating train examples...:  52% 62327/120000 [00:06<00:05, 10046.00 examples/s]\u001b[A\n",
            "Generating train examples...:  53% 63332/120000 [00:06<00:05, 9963.80 examples/s] \u001b[A\n",
            "Generating train examples...:  54% 64338/120000 [00:06<00:05, 9989.49 examples/s]\u001b[A\n",
            "Generating train examples...:  54% 65346/120000 [00:06<00:05, 10013.56 examples/s]\u001b[A\n",
            "Generating train examples...:  55% 66348/120000 [00:06<00:05, 9948.14 examples/s] \u001b[A\n",
            "Generating train examples...:  56% 67343/120000 [00:06<00:05, 9935.51 examples/s]\u001b[A\n",
            "Generating train examples...:  57% 68354/120000 [00:06<00:05, 9985.41 examples/s]\u001b[A\n",
            "Generating train examples...:  58% 69369/120000 [00:06<00:05, 10031.62 examples/s]\u001b[A\n",
            "Generating train examples...:  59% 70373/120000 [00:07<00:04, 10016.16 examples/s]\u001b[A\n",
            "Generating train examples...:  59% 71389/120000 [00:07<00:04, 10056.60 examples/s]\u001b[A\n",
            "Generating train examples...:  60% 72404/120000 [00:07<00:04, 10081.41 examples/s]\u001b[A\n",
            "Generating train examples...:  61% 73413/120000 [00:07<00:04, 10076.71 examples/s]\u001b[A\n",
            "Generating train examples...:  62% 74421/120000 [00:07<00:04, 10004.69 examples/s]\u001b[A\n",
            "Generating train examples...:  63% 75427/120000 [00:07<00:04, 10018.16 examples/s]\u001b[A\n",
            "Generating train examples...:  64% 76443/120000 [00:07<00:04, 10057.91 examples/s]\u001b[A\n",
            "Generating train examples...:  65% 77460/120000 [00:07<00:04, 10089.54 examples/s]\u001b[A\n",
            "Generating train examples...:  65% 78470/120000 [00:07<00:04, 10077.27 examples/s]\u001b[A\n",
            "Generating train examples...:  66% 79478/120000 [00:07<00:04, 10065.33 examples/s]\u001b[A\n",
            "Generating train examples...:  67% 80485/120000 [00:08<00:03, 10065.80 examples/s]\u001b[A\n",
            "Generating train examples...:  68% 81493/120000 [00:08<00:03, 10069.47 examples/s]\u001b[A\n",
            "Generating train examples...:  69% 82500/120000 [00:08<00:03, 9970.85 examples/s] \u001b[A\n",
            "Generating train examples...:  70% 83498/120000 [00:08<00:03, 9814.93 examples/s]\u001b[A\n",
            "Generating train examples...:  70% 84481/120000 [00:08<00:03, 9806.36 examples/s]\u001b[A\n",
            "Generating train examples...:  71% 85497/120000 [00:08<00:03, 9909.32 examples/s]\u001b[A\n",
            "Generating train examples...:  72% 86518/120000 [00:08<00:03, 9996.99 examples/s]\u001b[A\n",
            "Generating train examples...:  73% 87540/120000 [00:08<00:03, 10060.97 examples/s]\u001b[A\n",
            "Generating train examples...:  74% 88547/120000 [00:08<00:03, 10018.16 examples/s]\u001b[A\n",
            "Generating train examples...:  75% 89556/120000 [00:08<00:03, 10037.18 examples/s]\u001b[A\n",
            "Generating train examples...:  75% 90560/120000 [00:09<00:02, 10026.22 examples/s]\u001b[A\n",
            "Generating train examples...:  76% 91566/120000 [00:09<00:02, 10034.74 examples/s]\u001b[A\n",
            "Generating train examples...:  77% 92577/120000 [00:09<00:02, 10054.97 examples/s]\u001b[A\n",
            "Generating train examples...:  78% 93591/120000 [00:09<00:02, 10079.98 examples/s]\u001b[A\n",
            "Generating train examples...:  79% 94600/120000 [00:09<00:02, 9747.50 examples/s] \u001b[A\n",
            "Generating train examples...:  80% 95613/120000 [00:09<00:02, 9857.44 examples/s]\u001b[A\n",
            "Generating train examples...:  81% 96624/120000 [00:09<00:02, 9930.50 examples/s]\u001b[A\n",
            "Generating train examples...:  81% 97637/120000 [00:09<00:02, 9988.38 examples/s]\u001b[A\n",
            "Generating train examples...:  82% 98648/120000 [00:09<00:02, 10022.23 examples/s]\u001b[A\n",
            "Generating train examples...:  83% 99659/120000 [00:09<00:02, 10046.33 examples/s]\u001b[A\n",
            "Generating train examples...:  84% 100665/120000 [00:10<00:01, 10042.68 examples/s]\u001b[A\n",
            "Generating train examples...:  85% 101676/120000 [00:10<00:01, 10060.91 examples/s]\u001b[A\n",
            "Generating train examples...:  86% 102687/120000 [00:10<00:01, 10074.22 examples/s]\u001b[A\n",
            "Generating train examples...:  86% 103695/120000 [00:10<00:01, 9933.42 examples/s] \u001b[A\n",
            "Generating train examples...:  87% 104696/120000 [00:10<00:01, 9954.23 examples/s]\u001b[A\n",
            "Generating train examples...:  88% 105708/120000 [00:10<00:01, 10002.50 examples/s]\u001b[A\n",
            "Generating train examples...:  89% 106724/120000 [00:10<00:01, 10049.21 examples/s]\u001b[A\n",
            "Generating train examples...:  90% 107748/120000 [00:10<00:01, 10103.38 examples/s]\u001b[A\n",
            "Generating train examples...:  91% 108759/120000 [00:10<00:01, 10093.30 examples/s]\u001b[A\n",
            "Generating train examples...:  91% 109769/120000 [00:10<00:01, 10085.77 examples/s]\u001b[A\n",
            "Generating train examples...:  92% 110779/120000 [00:11<00:00, 10088.66 examples/s]\u001b[A\n",
            "Generating train examples...:  93% 111795/120000 [00:11<00:00, 10107.21 examples/s]\u001b[A\n",
            "Generating train examples...:  94% 112807/120000 [00:11<00:00, 10108.41 examples/s]\u001b[A\n",
            "Generating train examples...:  95% 113825/120000 [00:11<00:00, 10128.79 examples/s]\u001b[A\n",
            "Generating train examples...:  96% 114838/120000 [00:11<00:00, 10071.45 examples/s]\u001b[A\n",
            "Generating train examples...:  97% 115846/120000 [00:11<00:00, 10035.79 examples/s]\u001b[A\n",
            "Generating train examples...:  97% 116850/120000 [00:11<00:00, 10036.42 examples/s]\u001b[A\n",
            "Generating train examples...:  98% 117865/120000 [00:11<00:00, 10067.88 examples/s]\u001b[A\n",
            "Generating train examples...:  99% 118872/120000 [00:11<00:00, 10043.10 examples/s]\u001b[A\n",
            "Generating train examples...: 100% 119884/120000 [00:11<00:00, 10064.50 examples/s]\u001b[A\n",
            "                                                                                   \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-train.tfrecord*...:   0% 0/120000 [00:00<?, ? examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-train.tfrecord*...:   2% 2428/120000 [00:00<00:04, 24278.75 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-train.tfrecord*...:  41% 49766/120000 [00:00<00:00, 288440.00 examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-train.tfrecord*...:  81% 97594/120000 [00:00<00:00, 375113.42 examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-train.tfrecord*. Number of examples: 120000 (shards: [120000])\n",
            "Generating splits...:  50% 1/2 [00:12<00:12, 12.37s/ splits]\n",
            "Generating test examples...:   0% 0/7600 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating test examples...:  13% 981/7600 [00:00<00:00, 9806.85 examples/s]\u001b[A\n",
            "Generating test examples...:  26% 1962/7600 [00:00<00:00, 9424.55 examples/s]\u001b[A\n",
            "Generating test examples...:  39% 2961/7600 [00:00<00:00, 9676.73 examples/s]\u001b[A\n",
            "Generating test examples...:  52% 3989/7600 [00:00<00:00, 9910.00 examples/s]\u001b[A\n",
            "Generating test examples...:  66% 5005/7600 [00:00<00:00, 9996.04 examples/s]\u001b[A\n",
            "Generating test examples...:  79% 6026/7600 [00:00<00:00, 10065.00 examples/s]\u001b[A\n",
            "Generating test examples...:  93% 7033/7600 [00:00<00:00, 10050.58 examples/s]\u001b[A\n",
            "                                                                              \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-test.tfrecord*...:   0% 0/7600 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/ag_news_subset/1.0.0.incompleteW52J8R/ag_news_subset-test.tfrecord*. Number of examples: 7600 (shards: [7600])\n",
            "\u001b[1mDataset ag_news_subset downloaded and prepared to /root/tensorflow_datasets/ag_news_subset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[build.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='ag_news_subset',\n",
            "    full_name='ag_news_subset/1.0.0',\n",
            "    description=\"\"\"\n",
            "    AG is a collection of more than 1 million news articles. News articles have been\n",
            "    gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\n",
            "    activity. ComeToMyHead is an academic news search engine which has been running\n",
            "    since July, 2004. The dataset is provided by the academic comunity for research\n",
            "    purposes in data mining (clustering, classification, etc), information retrieval\n",
            "    (ranking, search, etc), xml, data compression, data streaming, and any other\n",
            "    non-commercial activity. For more information, please refer to the link\n",
            "    http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
            "    \n",
            "    The AG's news topic classification dataset is constructed by Xiang Zhang\n",
            "    (xiang.zhang@nyu.edu) from the dataset above. It is used as a text\n",
            "    classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\n",
            "    LeCun. Character-level Convolutional Networks for Text Classification. Advances\n",
            "    in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "    \n",
            "    The AG's news topic classification dataset is constructed by choosing 4 largest\n",
            "    classes from the original corpus. Each class contains 30,000 training samples\n",
            "    and 1,900 testing samples. The total number of training samples is 120,000 and\n",
            "    testing 7,600.\n",
            "    \"\"\",\n",
            "    homepage='https://arxiv.org/abs/1509.01626',\n",
            "    data_path=PosixGPath('/tmp/tmpqh6txl7dtfds'),\n",
            "    file_format=tfrecord,\n",
            "    download_size=11.24 MiB,\n",
            "    dataset_size=35.79 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'description': Text(shape=(), dtype=string),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=4),\n",
            "        'title': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('description', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=7600, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=120000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
            "        title={Character-level Convolutional Networks for Text Classification},\n",
            "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
            "        year={2015},\n",
            "        eprint={1509.01626},\n",
            "        archivePrefix={arXiv},\n",
            "        primaryClass={cs.LG}\n",
            "    }\"\"\",\n",
            ")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
        "\n",
        "dataset,info=\\\n",
        "tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32\n",
        "          , as_supervised=True)\n",
        "\n",
        "train_ds, val_ds, test_ds = dataset\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7Q6zlFRro_"
      },
      "source": [
        "## Preparing Integer Sequence Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g_Rc9pZs3lBo",
        "outputId": "79ff3e35-8535-44d1-d264-89f171bfdda7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AfTnCGIERrpA"
      },
      "outputs": [],
      "source": [
        "max_length = 50\n",
        "max_tokens = 2000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        "    standardize=custom_stopwords\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBL0CkaB5ukv"
      },
      "source": [
        "## Bi-directional RNN\n",
        "\n",
        "When translating in real-time, it would help to have access to worlds towards the end of a sentence, say, as well as earlier words in the sentence. One way to use the later words in the sentence is to feed the words into our RNN backward. So if we create two independent RNNs, we can feed one the words in their forward, or natural order, and the second gets their words in the revser order. This is the idea behind `Bi-directional RNNS`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuP7exeZ5ukw"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/BidirectionalRNN.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCNo3kUl5ukw"
      },
      "source": [
        "<img src=\"https://github.com/djp840/MSDS_458_Public/blob/master/images/Bidirectional2RNN.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN2wjSjdRrpB"
      },
      "source": [
        "## Sequence Model Built on One-Hot Encoded Vector Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq6dB-zWRrpB",
        "outputId": "9a649013-d4ee-4791-84d5-6f2e5978f025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 2000)        0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               520448    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 520,708\n",
            "Trainable params: 520,708\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "k.clear_session()\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"SparseCategoricalCrossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R64NWZQc5ukx"
      },
      "source": [
        "## One input is a sequence of integers.\n",
        "\n",
        "1. In order to keep a manageable input size, weâ€™ll truncate the inputs after the first 150 words. This is a reasonable choice, since the average review length is 233 words, and only 5% of reviews are longer than 150 words.\n",
        "\n",
        "2. Encode the integers into binary 500-dimensional vectors.\n",
        "\n",
        "3. Add a bidirectional LSTM.\n",
        "\n",
        "4. Finally, add a classification layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ognTeJM6RrpC"
      },
      "source": [
        "## Training Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kCbHrrsRrpC",
        "outputId": "10f0124c-3a48-4f46-a117-5af3ebbb473b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3563/3563 [==============================] - 29s 7ms/step - loss: 0.5328 - accuracy: 0.8162 - val_loss: 0.3719 - val_accuracy: 0.8747\n",
            "Epoch 2/5\n",
            "3563/3563 [==============================] - 25s 7ms/step - loss: 0.3823 - accuracy: 0.8734 - val_loss: 0.3531 - val_accuracy: 0.8795\n",
            "Epoch 3/5\n",
            "3563/3563 [==============================] - 26s 7ms/step - loss: 0.3600 - accuracy: 0.8809 - val_loss: 0.3464 - val_accuracy: 0.8793\n",
            "Epoch 4/5\n",
            "3563/3563 [==============================] - 26s 7ms/step - loss: 0.3477 - accuracy: 0.8846 - val_loss: 0.3408 - val_accuracy: 0.8818\n",
            "Epoch 5/5\n",
            "3563/3563 [==============================] - 26s 7ms/step - loss: 0.3375 - accuracy: 0.8873 - val_loss: 0.3395 - val_accuracy: 0.8848\n",
            "238/238 [==============================] - 2s 4ms/step - loss: 0.3474 - accuracy: 0.8841\n",
            "Test acc: 0.884\n",
            "CPU times: user 3min 26s, sys: 28.5 s, total: 3min 54s\n",
            "Wall time: 2min 13s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=5, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "MSDS458_Assignment_03_part02_SequenceModels_20220421_DEV_v4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}